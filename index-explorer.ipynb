{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NSW State Archives Index Explorer\n",
    "\n",
    "NSW State Archives provides a lot of rich descriptive data in its [online indexes](https://www.records.nsw.gov.au/archives/collections-and-research/guides-and-indexes/indexes-a-z). But there's so much data it can be hard to understand what's actually in each index. This notebook tries to help by generating an overview of an index, summarising the contents of each field.\n",
    "\n",
    "When you select an index from the dropdown list, the Index Explorer loads a CSV file containing [data harvested from the index](harvest-indexes.ipynb). It then looks at each column in the dataset, tries to identify the type of data inside, and attempts to tell you something useful about it.\n",
    "\n",
    "Given all the possible variations in recording and formatting data, there will be oddities and errors. But hopefully this will provide you with a useful starting point for further exploration.\n",
    "\n",
    "This notebook is designed to run in 'app mode', which hides all the code and de-clutters the interface. If you see lots of code below, look for an **Appmode** button in the menu bar above and click it. If there's no button, you can [run the Index Explorer using Binder](https://mybinder.org/v2/gh/GLAM-Workbench/nsw-state-archives/master?urlpath=%2Fapps%2Findex-explorer.ipynb).\n",
    "\n",
    "The Index Explorer is a slightly-modified version of the [GLAM CSV Explorer](https://glam-workbench.github.io/csv-explorer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from pandas.errors import ParserError\n",
    "import statistics\n",
    "import time\n",
    "import os\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.error import HTTPError\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import altair as alt\n",
    "from wordcloud import WordCloud\n",
    "from slugify import slugify\n",
    "alt.renderers.enable('notebook')\n",
    "alt.data_transformers.enable('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// This is necessary to stop the output area folding up\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {return false}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// This is necessary to stop the output area folding up\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {return false}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where the results go...\n",
    "results = widgets.Output()\n",
    "\n",
    "def read_csv(url, header=0, encoding=0):\n",
    "    '''\n",
    "    Loop through some encoding/parsing options to see if we can get the CSV to open properly.\n",
    "    '''\n",
    "    encodings = ['ISO-8859-1', 'latin-1']\n",
    "    headers = [None]\n",
    "    try:\n",
    "        if encoding > 0 and header > 0:\n",
    "            df = pd.read_csv(url, na_values=['-', ' '], encoding=encodings[encoding-1], header=headers[header-1])\n",
    "        elif encoding > 0:\n",
    "            df = pd.read_csv(url, na_values=['-', ' '], encoding=encodings[encoding-1])\n",
    "        elif header > 0:\n",
    "            df = pd.read_csv(url, na_values=['-', ' '], header=headers[header-1])\n",
    "        else:\n",
    "            df = pd.read_csv(url, na_values=['-', ' '])\n",
    "    except UnicodeDecodeError:\n",
    "        if encoding == len(encodings):\n",
    "            raise\n",
    "        else:\n",
    "            return read_csv(url=url, header=header, encoding=encoding+1)\n",
    "    except ParserError:\n",
    "        if header == len(headers):\n",
    "            raise\n",
    "        else:\n",
    "            return read_csv(url=url, header=header+1, encoding=encoding)\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def analyse_csv(b):\n",
    "    '''\n",
    "    Try to open the CSV file, and start the analysis.\n",
    "    '''\n",
    "    results.clear_output()\n",
    "    error = ''\n",
    "    with results:\n",
    "        title = select_csv.value\n",
    "        filename = '{}.csv'.format(slugify(title))\n",
    "        url = urljoin('https://raw.githubusercontent.com/wragge/srnsw-indexes/master/data/', filename)\n",
    "        try:\n",
    "            df = read_csv(url)\n",
    "        except UnicodeDecodeError:\n",
    "            error = 'Unicode error: unable to read the CSV!'\n",
    "        except ParserError:\n",
    "            error = 'Parser error: unable to read the CSV!'\n",
    "        except HTTPError:\n",
    "            error = 'File not found!'\n",
    "        html = '<hr><h2>{}</h2>'.format(title)\n",
    "        # html += '<h4>Source</h4><p><a href=\"{0}\">{0}</a></p>'.format(url)\n",
    "        if error:\n",
    "            html += '<p class=\"alert alert-warning\">{}</p>'.format(error)\n",
    "        display(HTML(html))\n",
    "        if not error:\n",
    "            rows, cols = df.shape\n",
    "            size = '<h4>Size</h4><ul>'\n",
    "            size += '<li>{} rows</li>'.format(rows)\n",
    "            size += '<li>{} columns</li></ul>'.format(cols)\n",
    "            cols = \"<h4>Columns</h4><ol>\"\n",
    "            for col in df.columns:\n",
    "                cols += '<li><a style=\"font-family: monospace\" href=\"#{}\">{}</a></li>'.format(slugify(col), col)\n",
    "            cols += '</ol>'\n",
    "            display(HTML(size))\n",
    "            display(HTML(cols))\n",
    "            display(HTML('<h4>Sample</h4>'))\n",
    "            display(df.head())\n",
    "            analyse_columns(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "date_cutoff = 0.8\n",
    "cutoff = 0.8\n",
    "unique_cutoff = 0.2\n",
    "\n",
    "def display_dates(df, col):\n",
    "    # Better to group years first, so that the altair data isn't huge\n",
    "    # Get counts by year\n",
    "    counts = df[col].groupby([df[col].dt.year]).agg('count').to_frame()\n",
    "    # Get the full range of years\n",
    "    years = pd.Index([y for y in range(int(counts.index[0]), int(counts.index[-1]) + 1)])\n",
    "    # Set missing years to zero\n",
    "    counts = counts.reindex(years, fill_value=0)\n",
    "    counts = counts.reset_index()\n",
    "    counts.columns = [col, 'count']\n",
    "    chart = alt.Chart(counts).mark_bar().encode(\n",
    "        x=alt.X('{}:O'.format(col), axis=alt.Axis(format='')),\n",
    "        y='count:Q',\n",
    "        tooltip=[alt.Tooltip('{}:O'.format(col), title='Year'), alt.Tooltip('count:Q', title='Count', format=',')],\n",
    "        color=alt.value('#5254a3')\n",
    "    ).properties(\n",
    "    )\n",
    "    display(chart)\n",
    "    \n",
    "def display_categories(df, col):\n",
    "    counts = df[col].value_counts()\n",
    "    if counts.size > 20:\n",
    "        counts = counts[:20].to_frame()\n",
    "    else:\n",
    "        counts = counts.to_frame()\n",
    "    counts = counts.reset_index()\n",
    "    counts.columns = [col, 'count']\n",
    "    chart = alt.Chart(counts).mark_bar().encode(\n",
    "        x='count:Q',\n",
    "        y=alt.Y('{}:N'.format(col),  sort=alt.EncodingSortField(field='count', op='count', order='ascending')),\n",
    "        tooltip=[alt.Tooltip('{}:N'.format(col), title='Category'), alt.Tooltip('count:Q', title='Count', format=',')],\n",
    "        color=alt.value('#8ca252')\n",
    "    )\n",
    "    display(chart)\n",
    "\n",
    "def display_wordcloud(df, col):\n",
    "    # Make a word cloud!\n",
    "    # The word cloud software splits the string into individual words and calculates their frquency\n",
    "    words = df[col].str.cat(sep=' ')\n",
    "    wordcloud = WordCloud(width=800, height=300, collocations=False).generate(words)\n",
    "    image = wordcloud.to_image()\n",
    "    image_file = 'images/{}_cloud_{}.png'.format(slugify(col), int(time.time()))\n",
    "    try:\n",
    "        image.save(image_file)\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs('images')\n",
    "        image.save(image_file)\n",
    "    display(HTML('<a href=\"{0}\"><img src=\"{0}\"></a>'.format(image_file)))\n",
    "\n",
    "def display_numbers(df, col, unique_count):\n",
    "    #display(df[col])\n",
    "    if unique_count <= 20:\n",
    "        # df[col].replace('0', np.NaN)\n",
    "        counts = df[col].value_counts().to_frame()\n",
    "        counts = counts.reset_index()\n",
    "        counts.columns = [col, 'count']\n",
    "        #display(counts)\n",
    "        chart = alt.Chart(counts).mark_bar().encode(\n",
    "            alt.X('{}:Q'.format(col)),\n",
    "            y='count',\n",
    "            tooltip=[alt.Tooltip('{}:Q'.format(col)), alt.Tooltip('count:Q', title='Count', format=',')],\n",
    "            color=alt.value('#ad494a')\n",
    "        )\n",
    "    else:\n",
    "        chart = alt.Chart(df).mark_bar().encode(\n",
    "            alt.X('{}:Q'.format(col), bin=alt.Bin(maxbins=10, nice=True)),\n",
    "            y='count()',\n",
    "            tooltip=[alt.Tooltip('{}:Q'.format(col), bin=alt.Bin(maxbins=10, nice=True), title='Range'), alt.Tooltip('count():Q', title='Count', format=',')],\n",
    "            color=alt.value('#ad494a')\n",
    "        )\n",
    "    display(chart)\n",
    "\n",
    "def text_field(df, col, value_count, word_counts, details):\n",
    "    html = 'This looks like a text field.'\n",
    "    display(HTML(html))\n",
    "    details['Total number of words'] = word_counts.sum()\n",
    "    details['Highest number of words'] = word_counts.max()\n",
    "    details['Median number of words'] = statistics.median(word_counts)\n",
    "    details['Number of empty records'] = df[col].shape[0] - value_count\n",
    "    display_details(details)\n",
    "    wordcloud = display_wordcloud(df, col)\n",
    "    image = wordcloud.to_image()\n",
    "    image_file = 'images/{}_cloud_{}.png'.format(slugify(col), int(time.time()))\n",
    "    try:\n",
    "        image.save(image_file)\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs('images')\n",
    "    display(HTML('<a href=\"{0}\"><img src=\"{0}\"></a>'.format(image_file)))\n",
    "    \n",
    "def textplus_field(df, col, value_count, unique_count, unique_ratio, word_counts, has_year, details, html):\n",
    "    median_word_count = statistics.median(word_counts)\n",
    "    mixed = False\n",
    "    details['Total number of words'] = word_counts.sum()\n",
    "    details['Highest number of words'] = word_counts.max()\n",
    "    details['Median number of words'] = median_word_count\n",
    "    details['Number of empty records'] = df[col].shape[0] - value_count\n",
    "    display_details(details)\n",
    "    has_mixed = df[col].str.contains(r'(?=\\S*[a-zA-Z\\/])(?=\\S*[0-9])', regex=True)\n",
    "    if has_mixed.sum() / value_count > cutoff and median_word_count <= 2:\n",
    "        mixed = True\n",
    "        html = '<p>This columns contains a small number of words that combine letters and numbers. They\\'re probably collection identifiers. Here\\'s some examples:</p><ul>'\n",
    "        samples = df.loc[df[col].notna()][col].sample(5).to_list()\n",
    "        for sample in samples:\n",
    "            html += '<li>{}</li>'.format(sample)\n",
    "        html += '</ul>'\n",
    "        display(HTML(html))\n",
    "    elif unique_ratio > (1 - cutoff):\n",
    "        if unique_count > 20:\n",
    "            try:\n",
    "                wordcloud = display_wordcloud(df, col)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        else:\n",
    "            display_categories(df, col)\n",
    "    else:\n",
    "        html = '<p>Only {:.2%} of the values in this column are unique, so it probably contains categories.</p>'.format(unique_ratio)\n",
    "        display(HTML(html))\n",
    "        display_categories(df, col)\n",
    "    has_number = df[col].str.contains(r'\\b\\d+\\b', regex=True)\n",
    "    # Check for dates\n",
    "    if has_year.sum() / value_count > cutoff and mixed is False:\n",
    "        html = '<p>Most of the values in this column include a number that looks like a year. It might be useful to convert them to dates.</p>'\n",
    "        df['{}_years_extracted'.format(col)] = df[col].str.extract(r'\\b(1[7-9]{1}\\d{2}|20[0-1]{1}\\d{1})\\b')\n",
    "        if df['{}_years_extracted'.format(col)].nunique(dropna=True) > 1:\n",
    "            df['{}_date_converted'.format(col)] = pd.to_datetime(df['{}_years_extracted'.format(col)], format='%Y', utc=True)\n",
    "            html += '<p>{:,} of {:,} values in this column were successfully parsed as dates.</p>'.format(df['{}_date_converted'.format(col)].dropna().size, value_count)\n",
    "            details = {}\n",
    "            details['Earliest date'] = df['{}_date_converted'.format(col)].min().strftime('%Y-%m-%d')\n",
    "            details['Latest date'] = df['{}_date_converted'.format(col)].max().strftime('%Y-%m-%d')\n",
    "            display(HTML(html))\n",
    "            display_details(details)\n",
    "            display_dates(df, '{}_date_converted'.format(col))\n",
    "    # Check for numbers\n",
    "    elif has_number.sum() / value_count > cutoff and mixed is False:\n",
    "        html = '<p>Most of the values in this column include a number. It might be useful to extract the values.</p>'\n",
    "        df['{}_numbers_extracted'.format(col)] = df[col].str.extract(r'\\b(\\d+)\\b')\n",
    "        if df['{}_numbers_extracted'.format(col)].nunique(dropna=True) > 2:\n",
    "            df['{}_numbers_extracted'.format(col)] = pd.to_numeric(df['{}_numbers_extracted'.format(col)], errors='coerce', downcast='integer')\n",
    "            details = {}\n",
    "            details['Highest value'] = df['{}_numbers_extracted'.format(col)].max()\n",
    "            details['Lowest value'] = df['{}_numbers_extracted'.format(col)].dropna().min()\n",
    "            display(HTML(html))\n",
    "            display_details(details)\n",
    "            display_numbers(df, '{}_numbers_extracted'.format(col), unique_count)\n",
    "        \n",
    "    \n",
    "def date_field(df, col, value_count, year_count, details, html):\n",
    "    default_dates = pd.to_datetime(df[col], infer_datetime_format=True, errors='coerce', utc=True)\n",
    "    default_dates_count = default_dates.dropna().size\n",
    "    dayfirst_dates = pd.to_datetime(df[col], infer_datetime_format=True, errors='coerce', dayfirst=True, yearfirst=True, utc=True)\n",
    "    dayfirst_dates_count = dayfirst_dates.dropna().size\n",
    "    if (default_dates_count / value_count > date_cutoff) and (default_dates_count >= dayfirst_dates_count):\n",
    "        df['{}_date_converted'.format(col)] = default_dates\n",
    "    elif (dayfirst_dates_count / value_count > date_cutoff) and (dayfirst_dates_count >= default_dates_count):\n",
    "        df['{}_date_converted'.format(col)] = dayfirst_dates\n",
    "    else:\n",
    "        # It's not a known date format, so let's just get the years\n",
    "        df['{}_years_extracted'.format(col)] = df[col].str.extract(r'\\b(1[7-9]{1}\\d{2}|20[0-1]{1}\\d{1})\\b')\n",
    "        df['{}_date_converted'.format(col)] = pd.to_datetime(df['{}_years_extracted'.format(col)], format='%Y', utc=True)\n",
    "    html += '<p>This looks like it contains dates.</p>'\n",
    "    html += '<p>{:,} of {:,} values in this column were successfully parsed as dates.</p>'.format(df['{}_date_converted'.format(col)].dropna().size, value_count)\n",
    "    details['Earliest date'] = df['{}_date_converted'.format(col)].min().strftime('%Y-%m-%d')\n",
    "    details['Latest date'] = df['{}_date_converted'.format(col)].max().strftime('%Y-%m-%d')\n",
    "    display(HTML(html))\n",
    "    display_details(details)\n",
    "    display_dates(df, '{}_date_converted'.format(col))\n",
    "\n",
    "def url_field(df, col, details, html):\n",
    "    display_details(details)\n",
    "    html += '<p>It looks like this column contains urls. Here are some examples:</p><ul>'\n",
    "    samples = df.loc[df[col].notna()][col].sample(5).to_list()\n",
    "    for sample in samples:\n",
    "        html += '<li><a href=\"{0}\">{0}</a></li>'.format(sample)\n",
    "    html += '</ul>'\n",
    "    display(HTML(html))\n",
    "    \n",
    "def unique_field(df, col, details, html):\n",
    "    display_details(details)\n",
    "    html += '<p>This column only contains one value:</p>'\n",
    "    html += '<blockquote>{}</blockquote>'.format(df[col].loc[df[col].first_valid_index()])\n",
    "    display(HTML(html))\n",
    "    \n",
    "def number_field(df, col, value_count, unique_count, unique_ratio, details, html):\n",
    "    has_year = df.loc[(df[col] >= 1700) & (df[col] <= 2019)]\n",
    "    if (has_year.size / value_count) > date_cutoff:\n",
    "        df['{}_date_converted'.format(col)] = pd.to_datetime(df[col], format='%Y', utc=True, errors='coerce')\n",
    "        html += '<p>This looks like it contains dates.</p>'\n",
    "        html += '<p>{:,} of {:,} values in this column were successfully parsed as dates.</p>'.format(df['{}_date_converted'.format(col)].dropna().size, value_count)\n",
    "        details['Earliest date'] = df['{}_date_converted'.format(col)].dropna().min().strftime('%Y-%m-%d')\n",
    "        details['Latest date'] = df['{}_date_converted'.format(col)].dropna().max().strftime('%Y-%m-%d')\n",
    "        display(HTML(html))\n",
    "        display_details(details)\n",
    "        display_dates(df, '{}_date_converted'.format(col))\n",
    "    else:\n",
    "        details['Highest value'] = df[col].max()\n",
    "        details['Lowest value'] = df[col].dropna().min()\n",
    "        display_details(details)\n",
    "        if unique_ratio > cutoff:\n",
    "            html = '{:.2%} of the values in this column are unique, so it\\'s probably some sort of identifier.'.format(unique_ratio)\n",
    "            display(HTML(html))\n",
    "        if unique_count <= 20:\n",
    "            display_categories(df, col)\n",
    "        else:\n",
    "            display_numbers(df, col, unique_count)\n",
    "        #Check for geocoordinates?\n",
    "\n",
    "def display_details(details):\n",
    "    details_df = pd.DataFrame.from_dict(details, orient='index', columns=[' '])\n",
    "    details_df.rename_axis('Summary', axis='columns', inplace=True)\n",
    "    details_df = details_df.style.set_table_styles([ dict(selector='th', props=[('text-align', 'left')] ) ])\n",
    "    display(details_df)\n",
    "\n",
    "def analyse_columns(df):\n",
    "    enriched_df = df.copy()\n",
    "    #out = widgets.Output()\n",
    "    outputs = {}\n",
    "    for index, col in enumerate(enriched_df.columns):\n",
    "        display(HTML('<hr><h3 id=\"{}\">{}. <code>{}</code></h3>'.format(slugify(col), index+1, col)))\n",
    "        details = {}\n",
    "        html = ''\n",
    "        # Are there any values in this column\n",
    "        value_count = enriched_df[col].dropna().size\n",
    "        details['Number of (non empty) values'] = '{:,} ({:.2%} of rows)'.format(value_count, (value_count / enriched_df[col].size))\n",
    "        if value_count:\n",
    "            # How many unique values are there in this column?\n",
    "            unique_count = enriched_df[col].nunique(dropna=True)\n",
    "            # What proportion of the values are unique?\n",
    "            unique_ratio = unique_count / value_count\n",
    "            details['Number of unique values'] = '{:,} ({:.2%} of non-empty values)'.format(unique_count, unique_ratio)\n",
    "            if unique_ratio == 1:\n",
    "                html += '<p>All the values in this column are unique, perhaps it''s some form of identifier.</p>'\n",
    "            if unique_count == 1:\n",
    "                unique_field(enriched_df, col, details, html)\n",
    "            # Check it's a string field\n",
    "            elif enriched_df[col].dtype == 'object':\n",
    "                word_counts = enriched_df[col].dropna().str.split().str.len().fillna(0)\n",
    "                median_word_count = statistics.median(word_counts)\n",
    "                # Check for the presence of years\n",
    "                # year_count = enriched_df[col].str.count(r'\\b1[7-9]{1}\\d{2}\\b|\\b20[0-1]{1}\\d{1}\\b').sum()\n",
    "                if enriched_df[col].str.startswith('http', na=False).sum() > 1:\n",
    "                    url_field(enriched_df, col, details, html)\n",
    "                #elif median_word_count <= 4:\n",
    "                    # How many have words that combine letters and numbers?\n",
    "                else:\n",
    "                    # How many start with words (and no numbers in the first two words)?\n",
    "                    starts_with_words = enriched_df[col].str.contains(r'^[a-zA-Z]+$|^(?:\\b[a-zA-Z]{2,}\\b\\W*){2}', regex=True)\n",
    "                    # How many have patterns that look like years?\n",
    "                    has_year = enriched_df[col].str.contains(r'\\b1[7-9]{1}\\d{2}|20[0-1]{1}\\d{1}\\b', regex=True)\n",
    "                    # If most don't start with words...\n",
    "                    # This filters out titles or names that might include dates.\n",
    "                    if (value_count - starts_with_words.sum()) / value_count > date_cutoff:\n",
    "                        # If most contain years...\n",
    "                        if (has_year.sum() / value_count) > date_cutoff:\n",
    "                            date_field(enriched_df, col, value_count, has_year.sum(), details, html)\n",
    "                        else:\n",
    "                            textplus_field(enriched_df, col, value_count, unique_count, unique_ratio, word_counts, has_year, details, html)\n",
    "                    else:\n",
    "                        textplus_field(enriched_df, col, value_count, unique_count, unique_ratio, word_counts, has_year, details, html)\n",
    "            elif enriched_df[col].dtype in ['int64', 'float64']:\n",
    "                number_field(enriched_df, col, value_count, unique_count, unique_ratio, details, html)\n",
    "        else:\n",
    "            html = 'This column is empty.'\n",
    "            display(HTML(html))\n",
    "            \n",
    "def clear_all(b):\n",
    "    select_csv.value = options[0][1]\n",
    "    csv_url.value = ''\n",
    "    results.clear_output()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c111ab295f4b0dbea57c2847fe6c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(HTML(value='Select an index:'), Dropdown(layout=Layout(width='80%'), options=('A…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csvs = pd.read_csv('https://raw.githubusercontent.com/wragge/srnsw-indexes/master/indexes.csv')\n",
    "csvs.sort_values(by=['title'], inplace=True)\n",
    "options = []\n",
    "for row in csvs.itertuples():\n",
    "    options.append(row.title)\n",
    "\n",
    "\n",
    "select_csv = widgets.Dropdown(\n",
    "        options=options,\n",
    "        description='',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "\n",
    "csv_url = widgets.Text(\n",
    "        placeholder='Enter the url of a CSV file',\n",
    "        description='Url:',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='100%')\n",
    "    )\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "        description='Clear',\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Clear current data',\n",
    "        icon=''\n",
    "    )\n",
    "\n",
    "analyse_button = widgets.Button(\n",
    "        description='Analyse CSV',\n",
    "        disabled=False,\n",
    "        button_style='primary', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltip='Analyse CSV',\n",
    "        icon=''\n",
    "    )\n",
    "\n",
    "clear_button.on_click(clear_all)\n",
    "analyse_button.on_click(analyse_csv)\n",
    "select_note = widgets.HTML('Select an index:')\n",
    "select_tab = widgets.VBox([select_note, select_csv])\n",
    "#tab = widgets.Tab(children=[select_tab])\n",
    "#tab.set_title(0, 'Select CSV')\n",
    "display(widgets.VBox([select_tab, widgets.HBox([analyse_button, clear_button]), results]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Created by [Tim Sherratt](https://timsherratt.org/).\n",
    "\n",
    "Part of the [GLAM Workbench](https://glam-workbench.github.io/) project.\n",
    "\n",
    "Work on this notebook was supported by the [Humanities, Arts and Social Sciences (HASS) Data Enhanced Virtual Lab](https://tinker.edu.au/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
